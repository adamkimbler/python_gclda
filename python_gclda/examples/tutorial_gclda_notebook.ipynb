{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tutorial Overview\n",
    "\n",
    "This file illustrates basic usage of the python_gclda toolbox. Specifically, we do the following steps:\n",
    "\n",
    "* Import the python modules\n",
    "* Build a python dataset object, and import data into the object from raw text files containing all data that the gcLDA model uses\n",
    "* Build a gclda model object instance\n",
    "* Train the gclda model object (using fewer iterations than should be used for actual modeling)\n",
    "* Export figures \n",
    "\n",
    "Note: this tutorial will assume that your working directory is the '/examples' subdirectory within the gclda package. If it is not, the relative paths to the datasets need to be modified for\tcreating the variable 'datasetLabel' below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 'gclda_dataset.py'\n",
      "Importing 'gclda_model.py'\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from python_gclda_package.gclda_dataset import gclda_dataset\n",
    "from python_gclda_package.gclda_model   import gclda_model\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset object instance and import data\n",
    "\n",
    "For this tutorial, we use a subset of 1000 documents from the neurosynth dataset. \n",
    "This will run faster, but produce sparser and noisier topics than the full dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Summary ---\n",
      "\t self.datasetLabel  = '2015Filtered2_1000docs'\n",
      "\t self.dataDirectory = '../datasets/neurosynth/'\n",
      "\t # word-types:   6755\n",
      "\t # word-indices: 40874\n",
      "\t # peak-indices: 34213\n",
      "\t # documents:    1000\n",
      "\t # peak-dims:    3\n"
     ]
    }
   ],
   "source": [
    "# Create dataset object instance: 'dat'\n",
    "# Inputs:\n",
    "datasetLabel  ='2015Filtered2_1000docs' # The directory name containing the dataset .txt files, which will be used as a 'dataset label'\n",
    "datasetDirectory =\t'../datasets/neurosynth/'# The relative path from the working directory to the root-directory containing the dataset folder\n",
    "dat = gclda_dataset(datasetLabel,datasetDirectory) # Create dataset object 'dat'\n",
    "# Import data from all files that are in dataset directory:\n",
    "dat.importAllData()\n",
    "# View dataset object after importing data:\n",
    "dat.displayDatasetSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a gclda model object\n",
    "\n",
    "For this tutorial, we will create gclda model instance, using T=100 topics, R=2 subregions per topic, and default values for all other hyper-parameters (See other sample scripts and documentation for details about all hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing GC-LDA Model\n",
      "Initializing GC-LDA Model\n",
      "--- Model Summary ---\n",
      " Current State:\n",
      "\t Current iteration   = 0\n",
      "\t Initialization Seed = 1\n",
      "\t Current Log-Likely  = -798038\n",
      " Model Hyper-Parameters:\n",
      "\t Symmetric = False\n",
      "\t nt    = 100\n",
      "\t nr    = 2\n",
      "\t alpha = 0.100\n",
      "\t beta  = 0.010\n",
      "\t gamma = 0.010\n",
      "\t delta = 1.000\n",
      "\t roi   = 50.000\n",
      "\t dobs  = 25\n",
      " Model Dataset-Object Information:\n",
      "\t # DatasetLabel       = 2015Filtered2_1000docs\n",
      "\t # Word-Tokens (nz)   = 40874\n",
      "\t # Peak-Tokens (ny)   = 34213\n",
      "\t # Word-Types (nw)    = 6755\n",
      "\t # Documents (nd)     = 1000\n",
      "\t Peak-Dimensionality  = 3\n"
     ]
    }
   ],
   "source": [
    "T = 100 # Number of topics\n",
    "R = 2 # Number of subregions in the gaussian mixture model used for each topic's spatial distribution\n",
    "\n",
    "# Create the model instance\n",
    "model = gclda_model(dat, T, R)\n",
    "# Randomly initialize the model\n",
    "model.initialize()\n",
    "# View the model after initialization:\n",
    "model.displayModelSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We will train for just a few iterations for the sake of time here (should take about 5 minutes). \n",
    "When training a full model, we recommend running for at least 1000 total iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 01 Log-likely: x =  -503509.4, w =  -285399.5, tot =  -788908.9\n",
      "Iter 02 Log-likely: x =  -502483.4, w =  -277888.2, tot =  -780371.6\n",
      "Iter 03 Log-likely: x =  -500573.4, w =  -273290.2, tot =  -773863.6\n",
      "Iter 04 Log-likely: x =  -498206.3, w =  -270078.0, tot =  -768284.2\n",
      "Iter 05 Log-likely: x =  -495543.3, w =  -267570.5, tot =  -763113.8\n",
      "Iter 06 Log-likely: x =  -492314.9, w =  -265750.5, tot =  -758065.4\n",
      "Iter 07 Log-likely: x =  -488836.8, w =  -264311.0, tot =  -753147.9\n",
      "Iter 08 Log-likely: x =  -485107.5, w =  -263155.8, tot =  -748263.3\n",
      "Iter 09 Log-likely: x =  -481350.9, w =  -262252.8, tot =  -743603.7\n",
      "Iter 10 Log-likely: x =  -478130.2, w =  -261581.0, tot =  -739711.2\n",
      "Iter 11 Log-likely: x =  -475200.8, w =  -260948.6, tot =  -736149.4\n",
      "Iter 12 Log-likely: x =  -472497.1, w =  -260441.0, tot =  -732938.2\n",
      "Iter 13 Log-likely: x =  -470148.4, w =  -259869.1, tot =  -730017.5\n",
      "Iter 14 Log-likely: x =  -468056.8, w =  -259500.9, tot =  -727557.7\n",
      "Iter 15 Log-likely: x =  -466114.4, w =  -259173.8, tot =  -725288.1\n",
      "Iter 16 Log-likely: x =  -464395.4, w =  -258601.3, tot =  -722996.8\n",
      "Iter 17 Log-likely: x =  -462767.6, w =  -258302.7, tot =  -721070.3\n",
      "Iter 18 Log-likely: x =  -461356.1, w =  -257906.0, tot =  -719262.1\n",
      "Iter 19 Log-likely: x =  -460164.9, w =  -257691.1, tot =  -717856.1\n",
      "Iter 20 Log-likely: x =  -459082.1, w =  -257498.5, tot =  -716580.6\n",
      "Iter 21 Log-likely: x =  -458155.0, w =  -257021.4, tot =  -715176.4\n",
      "Iter 22 Log-likely: x =  -457340.6, w =  -256790.8, tot =  -714131.4\n",
      "Iter 23 Log-likely: x =  -456622.4, w =  -256287.4, tot =  -712909.7\n",
      "Iter 24 Log-likely: x =  -456048.1, w =  -255958.1, tot =  -712006.3\n",
      "Iter 25 Log-likely: x =  -455390.6, w =  -255871.9, tot =  -711262.5\n"
     ]
    }
   ],
   "source": [
    "iterations = 25\n",
    "# During training, the model will print details about the model log-likelihood, etc., to the console.\n",
    "# The first parameter we pass controls how often we compute the log-likelihood (which slows inference down slightly)\n",
    "# The second parameter controls how much information gets printed to the console (0 = minimal, 2 = maximal) \n",
    "for i in range(iterations):\n",
    "    model.runCompleteIteration(1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export figures for trained model\n",
    "\n",
    "Here we export figures to file illustrating all topics in our trained model. Results will be placed into the subdirectories in the folder: 'examples/gclda_tutorial_results/' based on model parameter settings and how many iterations have run.\n",
    "\n",
    "Note that these topics will be **much** noiser than for a properly trained model (although some of the topics in these example results will capture known functional regions; e.g., topics 14/15). For results similar to those presented in our papers, you should\n",
    "* Use the complete dataset (and appropriate hyper-parameter settings)\n",
    "* Train until log-likelihood converges \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a rootdirectory to serve as a directory to store all tutorial results\n",
    "results_rootdir = 'gclda_tutorial_results' # We note that these results come from the tutorial, as opposed to the scripts for running full models\n",
    "if not os.path.isdir(results_rootdir):\n",
    "\tos.mkdir(results_rootdir)\n",
    "\n",
    "# We first use the method 'getModelDisplayString' to get a string identifier that is unique to the combination of:\n",
    "#  - DatasetLabel\n",
    "#  - All Model hyperparameters\n",
    "# This is useful for saving model output\n",
    "\n",
    "# Get model string identifier to use as a results directory\n",
    "modelString = model.getModelDisplayString()\n",
    "# Append the current model iteration to this directory name\n",
    "outputDirectory_data = results_rootdir + \"/\" + modelString + \"_Iteration_%d/\" % model.iter\n",
    "# We create a subdirectory of the outputdirectory to store figures\n",
    "outputDirectory_figures = outputDirectory_data + \"Figures/\"\n",
    "\n",
    "# Export some files giving topic-word distributions, as well as detailed accounts of all token->assignments\n",
    "model.printAllModelParams(outputDirectory_data)\n",
    "# Export Figures illustrating the spatial-extent and high probability word-types for the each topic in the model\n",
    "model.printTopicFigures(outputDirectory_figures, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
